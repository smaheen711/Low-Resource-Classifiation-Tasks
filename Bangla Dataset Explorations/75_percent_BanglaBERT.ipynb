{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tcGlbspToyg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyfdEsMlT-2Q"
      },
      "source": [
        "temp = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Dataset/final_dataset_75.xlsx')\n",
        "\n",
        "del temp['Unnamed: 0']\n",
        "temp['text'] = temp['Text']\n",
        "del temp['Text']\n",
        "temp['label'] = temp['Emotion']\n",
        "del temp['Emotion']\n",
        "\n",
        "# text = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Dataset/50_words_test.xlsx')\n",
        "# train = train.append(text)\n",
        "# train = train.sample(frac=1).reset_index(drop=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78cq9ovGWAoI"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(temp['text'], temp['label'], \n",
        "                                                                    random_state=42, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=temp['label'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=42, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57WxcAvBps_Z",
        "outputId": "926f9703-c530-4f0f-cb0c-133255ec2047"
      },
      "source": [
        "test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8953        happy\n",
              "1124         fear\n",
              "7892         fear\n",
              "20937         sad\n",
              "6965     surprise\n",
              "           ...   \n",
              "13136    surprise\n",
              "2296     surprise\n",
              "21862       angry\n",
              "4579        happy\n",
              "22476       angry\n",
              "Name: label, Length: 4050, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "68fefe9e275e452eb67dbdb9d50647e3",
            "93c62f2e4e9442b8850a60b7ca479389",
            "dd65eb8865534a1d83117cea16d14c6b",
            "a7457b9308f24051932520ed1982ebd7",
            "4dbc01935a284415964fd32b54147edf",
            "a841b5d25ed14aed9523e577fd0be26e",
            "80c7127fbda243faa54b3e51030e8d6f",
            "db9877ef5c644c48bc0757aaeee1bdc4",
            "3f5cb009cd7343bd9f696333b6b3217f",
            "3894a254e6e844f2976bb90bbdb0f369",
            "32a8e135cf084752bcdec293d62bde11",
            "bb2a1f063d2c44af91c330a1e42b6ffd",
            "7365bef096db4f1ea495682cdf8adfac",
            "4875873ba7224c24acf268feb57a73cd",
            "57e870ca242f4304a4ee53e4b4c1039a",
            "a3a48364cb724b419d8cc96bd8ffb0d0",
            "5f2387b070554409867610dd9041513d",
            "eb465aa632564492a6d4139157f3d6d1",
            "6ed990737283406897c03175939ae468",
            "ed52789aa31143feb53caf53ee57a37c",
            "416cfdc6ccf54780927e6f652cb332f2",
            "68e0ffdf808d42f982dde9eea2715b0d",
            "3b0d0ebc1c7f4c278a1a403ba22ca9d7",
            "c56268c3705d419db81264db8b983333",
            "53d34f7af8da40bd8e05ebb140f242b1",
            "e1bc35acf50b4510847ccc842e255622",
            "b19e556b93cc44199284b6f5c07f2fcd",
            "63624b68f5a54a899f9a5a914b413f07",
            "3528befeed5545eaa087447212ee4280",
            "192fb37e9487411db3a41c9dd9b61af7",
            "e581fa4f397e4278a1226e5c1c67ad16",
            "fe5ef9a7f98d403985873e5d719df2a9",
            "6b55224636284fc9a8fa1798193310f9"
          ]
        },
        "id": "AEXqA_9xWTAv",
        "outputId": "66175d36-4069-4f46-c8ce-20cd58054df9"
      },
      "source": [
        "# bert = AutoModel.from_pretrained('/content/drive/My Drive/Colab Notebooks/Dataset/bangla-bert-base')\n",
        "bert = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68fefe9e275e452eb67dbdb9d50647e3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/491 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb2a1f063d2c44af91c330a1e42b6ffd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/660M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b0d0ebc1c7f4c278a1a403ba22ca9d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "7dJchIN2jzut",
        "outputId": "1f67437b-73e7-45af-9159-c7fbbd9bfce5"
      },
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f72ccb63090>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6XgbhHHnTtl"
      },
      "source": [
        "max_seq_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoLqDE0inkfd",
        "outputId": "115a8cf3-9ce4-446f-c45c-96db706e363e"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7B_QRxHozIO"
      },
      "source": [
        "encoding = {\n",
        "    'happy': 0,\n",
        "    'angry': 1,\n",
        "    'sad': 2,\n",
        "    'disgust': 3,\n",
        "    'surprise': 4,\n",
        "    'fear': 5\n",
        "}\n",
        "\n",
        "train_labels =[encoding[x] for x in train_labels] \n",
        "test_labels = [encoding[x] for x in test_labels]\n",
        "val_labels = [encoding[x] for x in val_labels]\n",
        "# test_labels = [encoding[x] for x in train['Emotion']]\n",
        "# val_labels = [encoding[x] for x in train['Emotion']] \n",
        "# y_train = [encoding[x] for x in train['Emotion']]\n",
        "# y_test = [encoding[x] for x in train['Emotion']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGaST4B1nny0"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels)\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels)\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nI264VVopMx"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzRFTEOAprTE"
      },
      "source": [
        "# for param in bert.parameters():\n",
        "#     param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atj_Jmq3puGE"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,6)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEC_9chYpxbt"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9JIdILSp8Q7"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRY8MueqAA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98cf5827-b68e-42f0-e733-65a4074cb8a8"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGiINX7rqCer"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKb6f0fhqFXG"
      },
      "source": [
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.cpu().to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmFTI2PVqJZL"
      },
      "source": [
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWpVwR9-qN-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e560477-33a6-4c91-8d13-f43eec0c5234"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, temp = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, temp = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 1.387\n",
            "Validation Loss: 0.974\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.726\n",
            "Validation Loss: 0.779\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.449\n",
            "Validation Loss: 0.782\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.313\n",
            "Validation Loss: 0.737\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.238\n",
            "Validation Loss: 0.820\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.203\n",
            "Validation Loss: 0.801\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.174\n",
            "Validation Loss: 0.858\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.157\n",
            "Validation Loss: 0.911\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.137\n",
            "Validation Loss: 0.919\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    591.\n",
            "  Batch   100  of    591.\n",
            "  Batch   150  of    591.\n",
            "  Batch   200  of    591.\n",
            "  Batch   250  of    591.\n",
            "  Batch   300  of    591.\n",
            "  Batch   350  of    591.\n",
            "  Batch   400  of    591.\n",
            "  Batch   450  of    591.\n",
            "  Batch   500  of    591.\n",
            "  Batch   550  of    591.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    127.\n",
            "  Batch   100  of    127.\n",
            "\n",
            "Training Loss: 0.134\n",
            "Validation Loss: 1.030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky9UyM67vfeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2cfcd7-1ff9-4107-882f-468729ff54b5"
      },
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_ub534y5d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f61a94d-5d7c-4d5b-fdc1-9aefae484417"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeRmGQMXAIx9"
      },
      "source": [
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-KZwElwAJG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c51d0a-43f1-4d1b-a112-3a2de1199130"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.83       675\n",
            "           1       0.77      0.84      0.80       675\n",
            "           2       0.73      0.63      0.68       675\n",
            "           3       0.87      0.80      0.83       675\n",
            "           4       0.79      0.88      0.84       675\n",
            "           5       0.90      0.88      0.89       675\n",
            "\n",
            "    accuracy                           0.81      4050\n",
            "   macro avg       0.81      0.81      0.81      4050\n",
            "weighted avg       0.81      0.81      0.81      4050\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYLj0-uAAOXv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}