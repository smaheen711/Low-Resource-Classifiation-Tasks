{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "75_percent_mBERT_git.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6hmel6sKNSP"
      },
      "source": [
        "def reproduceResult():\n",
        "  seed_value= 0\n",
        "\n",
        "  \n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    ...\n",
        "\n",
        "\n",
        "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "  np.random.seed(0)\n",
        "  rn.seed(0)\n",
        "\n",
        "\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
        "                                          inter_op_parallelism_threads=1)\n",
        "\n",
        "\n",
        "  tf.compat.v1.set_random_seed(seed_value)\n",
        "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "  tf.compat.v1.keras.backend.clear_session()\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vURLkAC5_Jp0",
        "outputId": "0110afec-26ff-4c3a-aa7b-fbcd1a697256"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import os \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "from tensorflow import keras\n",
        "\n",
        "reproduceResult()\n",
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# tf.test.gpu_device_name()\n",
        "# from scipy import integrate\n",
        "# import os\n",
        "# import numpy as np\n",
        "# from tensorflow import keras\n",
        "import tempfile\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "# import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "from keras_lr_finder import LRFinder\n",
        "from clr_callback import CyclicLR\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "\n",
        "\n",
        "import keras_tuner\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from attention import Attention"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFgD7Seo_Xlq",
        "outputId": "4f15a9d6-64d5-40b0-e91c-3db8c8ca2be2"
      },
      "source": [
        "temp = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Datasets/final_dataset_75.xlsx')\n",
        "del temp['Unnamed: 0']\n",
        "\n",
        "train, test = train_test_split(temp, test_size=0.2, stratify = temp['Emotion'], random_state = 42)\n",
        "num_classes = 6\n",
        "embed_num_dims = 300\n",
        "max_seq_len = 50\n",
        "\n",
        "x_train = train['Text']\n",
        "x_test = test['Text']\n",
        "\n",
        "y_train = train['Emotion']\n",
        "y_test = test['Emotion']\n",
        "\n",
        "texts_train = x_train\n",
        "texts_test = x_test\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train['Text'])\n",
        "\n",
        "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "print('Number of unique words: {}'.format(len(index_of_words)))\n",
        "\n",
        "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len, padding='pre' )\n",
        "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len,  padding='pre')\n",
        "\n",
        "print(X_train_pad)\n",
        "\n",
        "\n",
        "encoding = {\n",
        "    'happy': 0,\n",
        "    'angry': 1,\n",
        "    'sad': 2,\n",
        "    'disgust': 3,\n",
        "    'surprise': 4,\n",
        "    'fear': 5\n",
        "}\n",
        "\n",
        "y_train = [encoding[x] for x in train['Emotion']]\n",
        "y_test = [encoding[x] for x in test['Emotion']]\n",
        "\n",
        "\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "def create_embedding_matrix(word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Datasets/cc.bn.300.vec') as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "print(embedd_matrix.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words: 26065\n",
            "[[    0     0     0 ...  3240    11  9549]\n",
            " [    0     0     0 ...    27  3065 13938]\n",
            " [    0     0     0 ...   256   494 13941]\n",
            " ...\n",
            " [    0     0     0 ...     0   113   234]\n",
            " [    0     0     0 ...  4526   244  3413]\n",
            " [    0     0     0 ...   169   857   131]]\n",
            "(26066, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGgsd5mMZPKn"
      },
      "source": [
        "# Random Search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IvOZoK8YGDI",
        "outputId": "0c109b3e-bf90-4f1f-fb8d-15f37e28252d"
      },
      "source": [
        "\n",
        "\n",
        "import time\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  \n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "  unit_attention = hp.Int(\"attention_unit\",min_value =32, max_value = 128, step = 16)\n",
        "  fake_val = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_unit = hp.Int(\"lstm_unit\",min_value =64, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =64, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  seq_input = keras.layers.Input(shape=(max_seq_len,))\n",
        "\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = max_seq_len,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  cnn = keras.layers.Conv1D(cnn_1_unit,3,kernel_regularizer='l2',bias_regularizer='l2',activity_regularizer='l2')(embedded)\n",
        "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
        "  cnn = keras.layers.BatchNormalization()(cnn)\n",
        "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
        "\n",
        "  attention_vec = keras.layers.TimeDistributed(keras.layers.Dense(unit_attention))(cnn)\n",
        "  attention_vec = keras.layers.Reshape((48,unit_attention))(attention_vec)\n",
        "  attention_vec = keras.layers.Activation('relu', name = 'cnn_attention_vec')(attention_vec)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn, attention_vec])\n",
        "\n",
        "\n",
        "  lstm = keras.layers.LSTM(lstm_unit, return_sequences=True,kernel_regularizer='l2',\n",
        "                           bias_regularizer='l2',activity_regularizer='l2',input_shape =(48,))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "  \n",
        "  \n",
        "\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3,kernel_regularizer='l2',\n",
        "                              bias_regularizer='l2',activity_regularizer='l2')(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(num_classes, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "\n",
        "clr_step_size = int((len(X_train_pad)/64))\n",
        "base_lr = 1e-3\n",
        "max_lr = 6e-3\n",
        "mode = 'exp_range'\n",
        "\n",
        "\n",
        "clr = CyclicLR(base_lr = base_lr, max_lr = max_lr, step_size = clr_step_size, mode = mode)\n",
        "\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 50,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "  \n",
        "tuner.search(x=X_train_pad,y = y_train,epochs = 20, batch_size = 64,callbacks = [stop,clr], \n",
        "             validation_data = (X_test_pad,y_test))\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 50 Complete [00h 08m 40s]\n",
            "val_accuracy: 0.19740740954875946\n",
            "\n",
            "Best val_accuracy So Far: 0.8251851797103882\n",
            "Total elapsed time: 14h 57m 10s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in 1627455737/untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 96\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 160\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.4\n",
            "Score: 0.8251851797103882\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 96\n",
            "cnn_1_unit: 48\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 160\n",
            "lstm_dropout: 0.1\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.8251851797103882\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 48\n",
            "cnn_1_unit: 64\n",
            "cnn_1_dropout: 0.1\n",
            "lstm_unit: 160\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 96\n",
            "cnn_2_dropout: 0.5\n",
            "Score: 0.8240740895271301\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 48\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 224\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 96\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.823888897895813\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 48\n",
            "cnn_1_unit: 64\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 96\n",
            "lstm_dropout: 0.1\n",
            "cnn_2_unit: 224\n",
            "cnn_2_dropout: 0.4\n",
            "Score: 0.8225926160812378\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 96\n",
            "cnn_1_unit: 80\n",
            "cnn_1_dropout: 0.30000000000000004\n",
            "lstm_unit: 224\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 128\n",
            "cnn_2_dropout: 0.4\n",
            "Score: 0.8216666579246521\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 80\n",
            "cnn_1_unit: 80\n",
            "cnn_1_dropout: 0.30000000000000004\n",
            "lstm_unit: 128\n",
            "lstm_dropout: 0.1\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.8196296095848083\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 64\n",
            "cnn_1_unit: 80\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 256\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.2\n",
            "Score: 0.8185185194015503\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 128\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.30000000000000004\n",
            "lstm_unit: 64\n",
            "lstm_dropout: 0.30000000000000004\n",
            "cnn_2_unit: 192\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.8161110877990723\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 128\n",
            "cnn_1_unit: 80\n",
            "cnn_1_dropout: 0.1\n",
            "lstm_unit: 256\n",
            "lstm_dropout: 0.4\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.8159258961677551\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}