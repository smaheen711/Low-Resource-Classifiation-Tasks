{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhhWW6n-zz4c"
      },
      "source": [
        "def reproduceResult():\n",
        "  seed_value= 0\n",
        "\n",
        "  \n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    ...\n",
        "\n",
        "\n",
        "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "  np.random.seed(0)\n",
        "  rn.seed(0)\n",
        "\n",
        "\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
        "                                          inter_op_parallelism_threads=1)\n",
        "\n",
        "\n",
        "  tf.compat.v1.set_random_seed(seed_value)\n",
        "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "  tf.compat.v1.keras.backend.clear_session()\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUQpEnCT0pXy",
        "outputId": "8c1f8bc5-b1a0-4d05-a973-811c02ba5991"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import os \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "from tensorflow import keras\n",
        "\n",
        "reproduceResult()\n",
        "import tempfile\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "from keras_lr_finder import LRFinder\n",
        "from clr_callback import CyclicLR\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "\n",
        "\n",
        "import keras_tuner\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCwXEmuZ0sQK",
        "outputId": "beb3315a-9747-4cbb-cf73-a483861627f1"
      },
      "source": [
        "reproduceResult()\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/IMDB/imdb_master.csv',encoding=\"latin-1\")\n",
        "\n",
        "# df2 = pd.read_csv('imdb_master.csv',encoding=\"latin-1\")\n",
        "df = df.drop(['Unnamed: 0','type','file'],axis=1)\n",
        "df.columns = [\"review\",\"sentiment\"]\n",
        "df = df[df.sentiment != 'unsup']\n",
        "df['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0})\n",
        "# df = pd.concat([df1, df2]).reset_index(drop=True)\n",
        "  \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "  \n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        " \n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if  not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "  \n",
        "df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "YwnMe6K_1XuA",
        "outputId": "dcaa9a79-d60a-413b-c983-499e09f58c2d"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Processed_Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>0</td>\n",
              "      <td>mr costner ha drag movie far longer necessary ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>0</td>\n",
              "      <td>example majority action film generic bore real...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>0</td>\n",
              "      <td>first hate moronic rapper couldnt act gun pres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>0</td>\n",
              "      <td>even beatles could write song everyone like al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>0</td>\n",
              "      <td>brass picture movie fit word really somewhat b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>Seeing as the vote average was pretty low, and...</td>\n",
              "      <td>1</td>\n",
              "      <td>see vote average wa pretty low fact clerk vide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>The plot had some wretched, unbelievable twist...</td>\n",
              "      <td>1</td>\n",
              "      <td>plot wretched unbelievable twist however chemi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am amazed at how this movie(and most others ...</td>\n",
              "      <td>1</td>\n",
              "      <td>amaze movieand others ha average 5 star lower ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>A Christmas Together actually came before my t...</td>\n",
              "      <td>1</td>\n",
              "      <td>christmas together actually come time ive rais...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>Working-class romantic drama from director Mar...</td>\n",
              "      <td>1</td>\n",
              "      <td>workingclass romantic drama director martin ri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  ...                                  Processed_Reviews\n",
              "0      Once again Mr. Costner has dragged out a movie...  ...  mr costner ha drag movie far longer necessary ...\n",
              "1      This is an example of why the majority of acti...  ...  example majority action film generic bore real...\n",
              "2      First of all I hate those moronic rappers, who...  ...  first hate moronic rapper couldnt act gun pres...\n",
              "3      Not even the Beatles could write songs everyon...  ...  even beatles could write song everyone like al...\n",
              "4      Brass pictures (movies is not a fitting word f...  ...  brass picture movie fit word really somewhat b...\n",
              "...                                                  ...  ...                                                ...\n",
              "49995  Seeing as the vote average was pretty low, and...  ...  see vote average wa pretty low fact clerk vide...\n",
              "49996  The plot had some wretched, unbelievable twist...  ...  plot wretched unbelievable twist however chemi...\n",
              "49997  I am amazed at how this movie(and most others ...  ...  amaze movieand others ha average 5 star lower ...\n",
              "49998  A Christmas Together actually came before my t...  ...  christmas together actually come time ive rais...\n",
              "49999  Working-class romantic drama from director Mar...  ...  workingclass romantic drama director martin ri...\n",
              "\n",
              "[50000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgRRRfX86QUi"
      },
      "source": [
        "data_95, data_5 = train_test_split(df, test_size=0.05,  random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yltf3kdl6ZDd"
      },
      "source": [
        "data_5 = data_5.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "data_5.to_csv(r'/content/drive/MyDrive/Colab Notebooks/Dataset/IMDB/imdb_5_percent.csv', index=True,header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zmdcHbmj788A",
        "outputId": "73811e34-1395-4575-984c-ccd6a6177705"
      },
      "source": [
        "data_5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Processed_Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Although the story is fictional, it draws from...</td>\n",
              "      <td>1</td>\n",
              "      <td>although story fictional draw reality history ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I had numerous problems with this film.&lt;br /&gt;&lt;...</td>\n",
              "      <td>0</td>\n",
              "      <td>numerous problem filmbr br contain basic factu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "      <td>hi people see wonderful movie im sure thet wou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I have just watched the season 2 finale of Doc...</td>\n",
              "      <td>1</td>\n",
              "      <td>watch season 2 finale doctor apart couple dull...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh dear. Some of the best talent in British TV...</td>\n",
              "      <td>0</td>\n",
              "      <td>oh dear best talent british tv make serial ass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>Home Room really surprised me. In comparison t...</td>\n",
              "      <td>1</td>\n",
              "      <td>home room really surprise comparison movie wri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>Too bad somebody did not have the smarts to re...</td>\n",
              "      <td>1</td>\n",
              "      <td>bad somebody smart release movie theater never...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>I know this movie is a low-budget horror movie...</td>\n",
              "      <td>0</td>\n",
              "      <td>know movie lowbudget horror movie intend favor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>Still funny after all these years. Midnight Ma...</td>\n",
              "      <td>1</td>\n",
              "      <td>still funny year midnight madness good enterta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>Back in 1983, Michael Jackson's popularity was...</td>\n",
              "      <td>1</td>\n",
              "      <td>back 1983 michael jackson popularity wa want m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review  ...                                  Processed_Reviews\n",
              "0     Although the story is fictional, it draws from...  ...  although story fictional draw reality history ...\n",
              "1     I had numerous problems with this film.<br /><...  ...  numerous problem filmbr br contain basic factu...\n",
              "2     hi for all the people who have seen this wonde...  ...  hi people see wonderful movie im sure thet wou...\n",
              "3     I have just watched the season 2 finale of Doc...  ...  watch season 2 finale doctor apart couple dull...\n",
              "4     Oh dear. Some of the best talent in British TV...  ...  oh dear best talent british tv make serial ass...\n",
              "...                                                 ...  ...                                                ...\n",
              "2495  Home Room really surprised me. In comparison t...  ...  home room really surprise comparison movie wri...\n",
              "2496  Too bad somebody did not have the smarts to re...  ...  bad somebody smart release movie theater never...\n",
              "2497  I know this movie is a low-budget horror movie...  ...  know movie lowbudget horror movie intend favor...\n",
              "2498  Still funny after all these years. Midnight Ma...  ...  still funny year midnight madness good enterta...\n",
              "2499  Back in 1983, Michael Jackson's popularity was...  ...  back 1983 michael jackson popularity wa want m...\n",
              "\n",
              "[2500 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2-twklwR7p9S",
        "outputId": "eab6df82-82c2-42d3-cb4f-21f16c9ece6c"
      },
      "source": [
        "temp = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/IMDB/imdb_5_percent.csv')\n",
        "\n",
        "del temp['Unnamed: 0']\n",
        "temp\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Processed_Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Although the story is fictional, it draws from...</td>\n",
              "      <td>1</td>\n",
              "      <td>although story fictional draw reality history ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I had numerous problems with this film.&lt;br /&gt;&lt;...</td>\n",
              "      <td>0</td>\n",
              "      <td>numerous problem filmbr br contain basic factu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "      <td>hi people see wonderful movie im sure thet wou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I have just watched the season 2 finale of Doc...</td>\n",
              "      <td>1</td>\n",
              "      <td>watch season 2 finale doctor apart couple dull...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh dear. Some of the best talent in British TV...</td>\n",
              "      <td>0</td>\n",
              "      <td>oh dear best talent british tv make serial ass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>Home Room really surprised me. In comparison t...</td>\n",
              "      <td>1</td>\n",
              "      <td>home room really surprise comparison movie wri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>Too bad somebody did not have the smarts to re...</td>\n",
              "      <td>1</td>\n",
              "      <td>bad somebody smart release movie theater never...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>I know this movie is a low-budget horror movie...</td>\n",
              "      <td>0</td>\n",
              "      <td>know movie lowbudget horror movie intend favor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>Still funny after all these years. Midnight Ma...</td>\n",
              "      <td>1</td>\n",
              "      <td>still funny year midnight madness good enterta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>Back in 1983, Michael Jackson's popularity was...</td>\n",
              "      <td>1</td>\n",
              "      <td>back 1983 michael jackson popularity wa want m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review  ...                                  Processed_Reviews\n",
              "0     Although the story is fictional, it draws from...  ...  although story fictional draw reality history ...\n",
              "1     I had numerous problems with this film.<br /><...  ...  numerous problem filmbr br contain basic factu...\n",
              "2     hi for all the people who have seen this wonde...  ...  hi people see wonderful movie im sure thet wou...\n",
              "3     I have just watched the season 2 finale of Doc...  ...  watch season 2 finale doctor apart couple dull...\n",
              "4     Oh dear. Some of the best talent in British TV...  ...  oh dear best talent british tv make serial ass...\n",
              "...                                                 ...  ...                                                ...\n",
              "2495  Home Room really surprised me. In comparison t...  ...  home room really surprise comparison movie wri...\n",
              "2496  Too bad somebody did not have the smarts to re...  ...  bad somebody smart release movie theater never...\n",
              "2497  I know this movie is a low-budget horror movie...  ...  know movie lowbudget horror movie intend favor...\n",
              "2498  Still funny after all these years. Midnight Ma...  ...  still funny year midnight madness good enterta...\n",
              "2499  Back in 1983, Michael Jackson's popularity was...  ...  back 1983 michael jackson popularity wa want m...\n",
              "\n",
              "[2500 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeHpux5P4sFt"
      },
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(temp['Processed_Reviews'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(temp['Processed_Reviews'])\n",
        "  \n",
        "RNN_CELL_SIZE = 32\n",
        "  \n",
        "MAX_LEN = 130  \n",
        "X_train = pad_sequences(list_tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train = temp['sentiment']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7vxGO3H48gX"
      },
      "source": [
        "X_train, X_test = train_test_split(X_train, test_size=0.2,  random_state = 42)\n",
        "y_train, y_test = train_test_split(y_train, test_size=0.2,  random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSRY7x4_5aYc",
        "outputId": "e3e4507f-5ff5-4811-e717-d1efb118ffab"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 130)\n",
            "(500, 130)\n",
            "(2000,)\n",
            "(500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La32sYmf48Fw"
      },
      "source": [
        "def create_embedding_matrix(word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Dataset/cc.en.300.vec') as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6atJr8ym5pBd",
        "outputId": "4d0730f8-cdd7-430d-8ac6-03b86599cea9"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "import time\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  \n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "  unit_attention = hp.Int(\"attention_unit\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_unit = hp.Int(\"lstm_unit\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "\n",
        "\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  cnn = keras.layers.Conv1D(cnn_1_unit,3)(embedded)\n",
        "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
        "  cnn = keras.layers.BatchNormalization()(cnn)\n",
        "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
        "\n",
        "  attention_vec = keras.layers.TimeDistributed(keras.layers.Dense(unit_attention))(cnn)\n",
        "  attention_vec = keras.layers.Reshape((128,unit_attention))(attention_vec)\n",
        "  attention_vec = keras.layers.Activation('relu', name = 'cnn_attention_vec')(attention_vec)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn, attention_vec])\n",
        "\n",
        "\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_unit, \n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "  \n",
        "  \n",
        "\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(2, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# clr_step_size = int((len(X_train)/32))\n",
        "# base_lr = 2e-5\n",
        "# max_lr = 1e-3\n",
        "# mode = 'exp_range'\n",
        "\n",
        "\n",
        "# clr = CyclicLR(base_lr = base_lr, max_lr = max_lr, step_size = clr_step_size, mode = mode)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 30,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "  \n",
        "tuner.search(x=X_train,y = y_train,epochs = 20, batch_size = 50, validation_data = (X_test,y_test),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 03m 10s]\n",
            "val_accuracy: 0.8159999847412109\n",
            "\n",
            "Best val_accuracy So Far: 0.8479999899864197\n",
            "Total elapsed time: 02h 43m 13s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in 1632298135/untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 64\n",
            "cnn_1_unit: 80\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 160\n",
            "lstm_dropout: 0.1\n",
            "cnn_2_unit: 32\n",
            "cnn_2_dropout: 0.2\n",
            "Score: 0.8479999899864197\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 96\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 160\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 128\n",
            "cnn_2_dropout: 0.4\n",
            "Score: 0.8379999995231628\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 112\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 160\n",
            "lstm_dropout: 0.2\n",
            "cnn_2_unit: 128\n",
            "cnn_2_dropout: 0.5\n",
            "Score: 0.8360000252723694\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 112\n",
            "cnn_1_unit: 48\n",
            "cnn_1_dropout: 0.1\n",
            "lstm_unit: 128\n",
            "lstm_dropout: 0.5\n",
            "cnn_2_unit: 128\n",
            "cnn_2_dropout: 0.4\n",
            "Score: 0.8320000171661377\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 32\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 32\n",
            "lstm_dropout: 0.1\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.8299999833106995\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 128\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.30000000000000004\n",
            "lstm_unit: 32\n",
            "lstm_dropout: 0.30000000000000004\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.828000009059906\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 80\n",
            "cnn_1_unit: 48\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 96\n",
            "lstm_dropout: 0.30000000000000004\n",
            "cnn_2_unit: 128\n",
            "cnn_2_dropout: 0.2\n",
            "Score: 0.8259999752044678\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 112\n",
            "cnn_1_unit: 64\n",
            "cnn_1_dropout: 0.30000000000000004\n",
            "lstm_unit: 96\n",
            "lstm_dropout: 0.30000000000000004\n",
            "cnn_2_unit: 64\n",
            "cnn_2_dropout: 0.2\n",
            "Score: 0.8259999752044678\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 80\n",
            "cnn_1_unit: 80\n",
            "cnn_1_dropout: 0.1\n",
            "lstm_unit: 192\n",
            "lstm_dropout: 0.4\n",
            "cnn_2_unit: 160\n",
            "cnn_2_dropout: 0.30000000000000004\n",
            "Score: 0.8259999752044678\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "attention_unit: 32\n",
            "cnn_1_unit: 32\n",
            "cnn_1_dropout: 0.2\n",
            "lstm_unit: 64\n",
            "lstm_dropout: 0.30000000000000004\n",
            "cnn_2_unit: 224\n",
            "cnn_2_dropout: 0.1\n",
            "Score: 0.8240000009536743\n"
          ]
        }
      ]
    }
  ]
}